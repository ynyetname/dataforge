{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SATELLITE IMAGERY BASED PROPERTY VALUATION - PREPROCESSING\n",
    "\n",
    "This notebook preprocesses the property data and satellite images for the CurbAppeal.net competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_curbappealnet.csv')\n",
    "test_df = pd.read_csv('test_curbappealnet.csv')\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"\\nNote: Test data has no 'price' column (to be predicted)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "print(train_df.info())\n",
    "print(\"\\nMissing values:\")\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(train_df['price'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Price Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(np.log1p(train_df['price']), bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "plt.xlabel('Log(Price + 1)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Log-Transformed Price Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.boxplot(train_df['price'])\n",
    "plt.ylabel('Price')\n",
    "plt.title('Price Boxplot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('price_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nPrice Statistics:\")\n",
    "print(f\"Mean: ${train_df['price'].mean():,.2f}\")\n",
    "print(f\"Median: ${train_df['price'].median():,.2f}\")\n",
    "print(f\"Std Dev: ${train_df['price'].std():,.2f}\")\n",
    "print(f\"Min: ${train_df['price'].min():,.2f}\")\n",
    "print(f\"Max: ${train_df['price'].max():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = train_df[numeric_cols].corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top correlations with price\n",
    "price_corr = correlation_matrix['price'].sort_values(ascending=False)\n",
    "print(\"\\nTop 10 features correlated with price:\")\n",
    "print(price_corr[1:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "features_to_plot = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', \n",
    "                    'floors', 'view', 'condition', 'grade', 'yr_built']\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    row, col = idx // 3, idx % 3\n",
    "    axes[row, col].hist(train_df[feature], bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[row, col].set_xlabel(feature)\n",
    "    axes[row, col].set_ylabel('Frequency')\n",
    "    axes[row, col].set_title(f'{feature} Distribution')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(train_df['long'], train_df['lat'], \n",
    "                     c=train_df['price'], cmap='viridis', \n",
    "                     alpha=0.5, s=10)\n",
    "plt.colorbar(scatter, label='Price')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Property Locations Colored by Price')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "zipcode_price = train_df.groupby('zipcode')['price'].mean().sort_values(ascending=False).head(15)\n",
    "plt.barh(range(len(zipcode_price)), zipcode_price.values)\n",
    "plt.yticks(range(len(zipcode_price)), zipcode_price.index)\n",
    "plt.xlabel('Average Price')\n",
    "plt.ylabel('Zipcode')\n",
    "plt.title('Top 15 Zipcodes by Average Price')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('geographic_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price vs Key Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Price vs Sqft Living\n",
    "axes[0, 0].scatter(train_df['sqft_living'], train_df['price'], alpha=0.5, s=10)\n",
    "axes[0, 0].set_xlabel('Sqft Living')\n",
    "axes[0, 0].set_ylabel('Price')\n",
    "axes[0, 0].set_title('Price vs Sqft Living')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Price vs Grade\n",
    "train_df.boxplot(column='price', by='grade', ax=axes[0, 1])\n",
    "axes[0, 1].set_xlabel('Grade')\n",
    "axes[0, 1].set_ylabel('Price')\n",
    "axes[0, 1].set_title('Price vs Grade')\n",
    "axes[0, 1].get_figure().suptitle('')\n",
    "\n",
    "# Price vs Waterfront\n",
    "train_df.boxplot(column='price', by='waterfront', ax=axes[1, 0])\n",
    "axes[1, 0].set_xlabel('Waterfront')\n",
    "axes[1, 0].set_ylabel('Price')\n",
    "axes[1, 0].set_title('Price vs Waterfront')\n",
    "axes[1, 0].get_figure().suptitle('')\n",
    "\n",
    "# Price vs View\n",
    "train_df.boxplot(column='price', by='view', ax=axes[1, 1])\n",
    "axes[1, 1].set_xlabel('View')\n",
    "axes[1, 1].set_ylabel('Price')\n",
    "axes[1, 1].set_title('Price vs View')\n",
    "axes[1, 1].get_figure().suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('price_vs_features.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Explore Satellite Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SATELLITE IMAGES ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define image paths - UPDATED TEST PATH\n",
    "TRAIN_IMAGE_DIR = '1767547761150_satellite_ima'  \n",
    "TEST_IMAGE_DIR = r'C:\\dataset_curbappealnet\\satellite_ima\\test_images'   # UPDATED PATH\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_path(property_id, image_dir):\n",
    "    \"\"\"Get image path for a given property ID\"\"\"\n",
    "    for ext in ['.jpg', '.png', '.jpeg']:\n",
    "        img_path = os.path.join(image_dir, f\"{int(property_id)}{ext}\")\n",
    "        if os.path.exists(img_path):\n",
    "            return img_path\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Training Image Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available TRAINING images\n",
    "available_images = []\n",
    "missing_images = []\n",
    "\n",
    "print(\"Checking TRAINING image availability...\")\n",
    "for idx, property_id in enumerate(train_df['id']):\n",
    "    img_path = get_image_path(property_id, TRAIN_IMAGE_DIR)\n",
    "    if img_path:\n",
    "        available_images.append((property_id, img_path))\n",
    "    else:\n",
    "        missing_images.append(property_id)\n",
    "    \n",
    "    if (idx + 1) % 1000 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(train_df)} properties...\")\n",
    "\n",
    "print(f\"\\nTotal properties: {len(train_df)}\")\n",
    "print(f\"Available images: {len(available_images)}\")\n",
    "print(f\"Missing images: {len(missing_images)}\")\n",
    "print(f\"Coverage: {len(available_images)/len(train_df)*100:.2f}%\")\n",
    "\n",
    "train_df_with_images = train_df[train_df['id'].isin([img[0] for img in available_images])].copy()\n",
    "print(f\"Filtered training dataset shape: {train_df_with_images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Test Image Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Checking TEST images availability...\")\n",
    "test_available_images = []\n",
    "test_missing_images = []\n",
    "\n",
    "for idx, property_id in enumerate(test_df['id']):\n",
    "    img_path = get_image_path(property_id, TEST_IMAGE_DIR)\n",
    "    if img_path:\n",
    "        test_available_images.append((property_id, img_path))\n",
    "    else:\n",
    "        test_missing_images.append(property_id)\n",
    "    \n",
    "    if (idx + 1) % 1000 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(test_df)} test properties...\")\n",
    "\n",
    "print(f\"\\nTest Dataset:\")\n",
    "print(f\"Total test properties: {len(test_df)}\")\n",
    "print(f\"Available test images: {len(test_available_images)}\")\n",
    "print(f\"Missing test images: {len(test_missing_images)}\")\n",
    "print(f\"Coverage: {len(test_available_images)/len(test_df)*100:.2f}%\")\n",
    "\n",
    "test_df_with_images = test_df[test_df['id'].isin([img[0] for img in test_available_images])].copy()\n",
    "print(f\"Filtered test dataset shape: {test_df_with_images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images\n",
    "print(\"\\nDisplaying sample satellite images...\")\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "sample_indices = np.random.choice(len(available_images), 12, replace=False)\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    property_id, img_path = available_images[sample_indices[idx]]\n",
    "    img = Image.open(img_path)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    price = train_df_with_images[train_df_with_images['id'] == property_id]['price'].values[0]\n",
    "    ax.set_title(f'ID: {int(property_id)}\\nPrice: ${price:,.0f}', fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_satellite_images.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check image dimensions\n",
    "print(\"\\nAnalyzing image dimensions...\")\n",
    "image_dimensions = []\n",
    "for i, (property_id, img_path) in enumerate(available_images[:100]):\n",
    "    img = Image.open(img_path)\n",
    "    image_dimensions.append(img.size)\n",
    "\n",
    "unique_dimensions = set(image_dimensions)\n",
    "print(f\"Unique image dimensions found: {unique_dimensions}\")\n",
    "print(f\"Most common dimension: {max(set(image_dimensions), key=image_dimensions.count)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering from Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING - TABULAR DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create new features from existing ones\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Age of property\n",
    "    df['age'] = 2025 - df['yr_built']\n",
    "    df['age_after_renovation'] = np.where(df['yr_renovated'] > 0, \n",
    "                                          2025 - df['yr_renovated'], \n",
    "                                          df['age'])\n",
    "    \n",
    "    # Boolean features\n",
    "    df['is_renovated'] = (df['yr_renovated'] > 0).astype(int)\n",
    "    df['has_basement'] = (df['sqft_basement'] > 0).astype(int)\n",
    "    \n",
    "    # Ratio features\n",
    "    if 'price' in df.columns:\n",
    "        df['price_per_sqft'] = df['price'] / df['sqft_living']\n",
    "    df['sqft_living_ratio'] = df['sqft_living'] / df['sqft_lot']\n",
    "    df['sqft_above_ratio'] = df['sqft_above'] / df['sqft_living']\n",
    "    df['bedroom_bathroom_ratio'] = df['bedrooms'] / (df['bathrooms'] + 1)\n",
    "    \n",
    "    # Living space features\n",
    "    df['total_rooms'] = df['bedrooms'] + df['bathrooms']\n",
    "    df['sqft_per_room'] = df['sqft_living'] / (df['total_rooms'] + 1)\n",
    "    \n",
    "    # Neighborhood features\n",
    "    df['neighborhood_quality'] = (df['sqft_living15'] + df['sqft_lot15']) / 2\n",
    "    \n",
    "    # Location-based features\n",
    "    df['lat_long_interaction'] = df['lat'] * df['long']\n",
    "    \n",
    "    # Binning features\n",
    "    if 'price' in df.columns:\n",
    "        df['price_category'] = pd.cut(df['price'], \n",
    "                                       bins=[0, 200000, 400000, 600000, 1000000, float('inf')],\n",
    "                                       labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
    "    \n",
    "    df['grade_category'] = pd.cut(df['grade'],\n",
    "                                   bins=[0, 6, 8, 10, float('inf')],\n",
    "                                   labels=['poor', 'average', 'good', 'excellent'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering to training data\n",
    "train_df_engineered = engineer_features(train_df_with_images)\n",
    "print(f\"Training - Original features: {train_df_with_images.shape[1]}\")\n",
    "print(f\"Training - After feature engineering: {train_df_engineered.shape[1]}\")\n",
    "print(f\"\\nNew features created:\")\n",
    "new_features = set(train_df_engineered.columns) - set(train_df_with_images.columns)\n",
    "for feat in new_features:\n",
    "    print(f\"  - {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply same feature engineering to test data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING - TEST DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Add dummy price column for test data (needed for engineer_features function)\n",
    "test_df_with_images['price'] = 0\n",
    "test_df_engineered = engineer_features(test_df_with_images)\n",
    "# Remove price-dependent features\n",
    "test_df_engineered = test_df_engineered.drop(['price', 'price_per_sqft', 'price_category'], axis=1)\n",
    "\n",
    "print(f\"Test - After feature engineering: {test_df_engineered.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for infinite values\n",
    "print(\"\\nChecking for infinite values in training data...\")\n",
    "inf_cols = []\n",
    "for col in train_df_engineered.select_dtypes(include=[np.number]).columns:\n",
    "    if np.isinf(train_df_engineered[col]).any():\n",
    "        inf_cols.append(col)\n",
    "        print(f\"  - {col}: {np.isinf(train_df_engineered[col]).sum()} infinite values\")\n",
    "\n",
    "# Replace infinite values with NaN and then fill\n",
    "if inf_cols:\n",
    "    train_df_engineered[inf_cols] = train_df_engineered[inf_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    train_df_engineered[inf_cols] = train_df_engineered[inf_cols].fillna(train_df_engineered[inf_cols].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMAGE PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def load_and_preprocess_image(img_path, target_size=(224, 224)):\n",
    "    \"\"\"Load and preprocess a single image\"\"\"\n",
    "    try:\n",
    "        img = load_img(img_path, target_size=target_size)\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = img_array / 255.0  # Normalize to [0, 1]\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_image_dataset(df, image_dir, img_size=224):\n",
    "    \"\"\"Create image dataset from dataframe\"\"\"\n",
    "    images = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    print(f\"Loading {len(df)} images...\")\n",
    "    for idx, row in df.iterrows():\n",
    "        img_path = get_image_path(row['id'], image_dir)\n",
    "        if img_path:\n",
    "            img_array = load_and_preprocess_image(img_path, target_size=(img_size, img_size))\n",
    "            if img_array is not None:\n",
    "                images.append(img_array)\n",
    "                valid_indices.append(idx)\n",
    "        \n",
    "        if (len(images)) % 500 == 0:\n",
    "            print(f\"  Loaded {len(images)} images...\")\n",
    "    \n",
    "    print(f\"Successfully loaded {len(images)} images\")\n",
    "    return np.array(images), valid_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training images\n",
    "X_images, valid_indices = create_image_dataset(train_df_engineered, TRAIN_IMAGE_DIR, IMG_SIZE)\n",
    "print(f\"\\nTraining image dataset shape: {X_images.shape}\")\n",
    "\n",
    "# Filter dataframe to match loaded images\n",
    "train_df_final = train_df_engineered.loc[valid_indices].reset_index(drop=True)\n",
    "print(f\"Final training dataset shape: {train_df_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test images\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING TEST IMAGES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_images_test, valid_test_indices = create_image_dataset(test_df_engineered, TEST_IMAGE_DIR, IMG_SIZE)\n",
    "print(f\"\\nTest image dataset shape: {X_images_test.shape}\")\n",
    "\n",
    "# Filter test dataframe\n",
    "test_df_final = test_df_engineered.loc[valid_test_indices].reset_index(drop=True)\n",
    "print(f\"Final test dataset shape: {test_df_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize preprocessed images\n",
    "print(\"\\nVisualizing preprocessed images...\")\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "sample_indices = np.random.choice(len(X_images), 8, replace=False)\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_images[sample_indices[idx]])\n",
    "    price = train_df_final.iloc[sample_indices[idx]]['price']\n",
    "    ax.set_title(f'Price: ${price:,.0f}', fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('preprocessed_images.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deep Feature Extraction from Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEEP FEATURE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def extract_features_with_model(images, model_name='efficientnet'):\n",
    "    \"\"\"Extract deep features from images using pre-trained models\"\"\"\n",
    "    \n",
    "    print(f\"\\nExtracting features using {model_name}...\")\n",
    "    \n",
    "    if model_name == 'efficientnet':\n",
    "        base_model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')\n",
    "        preprocess_func = efficientnet_preprocess\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    # Preprocess images for the specific model\n",
    "    processed_images = preprocess_func(images * 255.0)  # Convert back to [0, 255]\n",
    "    \n",
    "    # Extract features in batches\n",
    "    batch_size = 32\n",
    "    features_list = []\n",
    "    \n",
    "    for i in range(0, len(processed_images), batch_size):\n",
    "        batch = processed_images[i:i+batch_size]\n",
    "        batch_features = base_model.predict(batch, verbose=0)\n",
    "        features_list.append(batch_features)\n",
    "        \n",
    "        if (i + batch_size) % 500 == 0:\n",
    "            print(f\"  Processed {min(i+batch_size, len(processed_images))}/{len(processed_images)} images...\")\n",
    "    \n",
    "    features = np.vstack(features_list)\n",
    "    print(f\"Extracted features shape: {features.shape}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from TRAINING images\n",
    "print(\"Using EfficientNetB0 for feature extraction on TRAINING images...\")\n",
    "image_features = extract_features_with_model(X_images, model_name='efficientnet')\n",
    "\n",
    "# Create feature names\n",
    "image_feature_names = [f'img_feat_{i}' for i in range(image_features.shape[1])]\n",
    "image_features_df = pd.DataFrame(image_features, columns=image_feature_names)\n",
    "\n",
    "print(f\"\\nTraining image features shape: {image_features_df.shape}\")\n",
    "print(f\"Sample features:\")\n",
    "print(image_features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from TEST images\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTING FEATURES FROM TEST IMAGES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Using EfficientNetB0 for feature extraction on TEST images...\")\n",
    "test_image_features = extract_features_with_model(X_images_test, model_name='efficientnet')\n",
    "\n",
    "# Create test image features dataframe\n",
    "test_image_features_df = pd.DataFrame(test_image_features, columns=image_feature_names)\n",
    "\n",
    "print(f\"\\nTest image features shape: {test_image_features_df.shape}\")\n",
    "print(f\"Sample features:\")\n",
    "print(test_image_features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Combine Features and Prepare Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMBINING FEATURES - TRAINING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select relevant tabular features\n",
    "tabular_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
    "                   'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n",
    "                   'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n",
    "                   'lat', 'long', 'sqft_living15', 'sqft_lot15',\n",
    "                   'age', 'age_after_renovation', 'is_renovated', 'has_basement',\n",
    "                   'sqft_living_ratio', 'sqft_above_ratio', 'bedroom_bathroom_ratio',\n",
    "                   'total_rooms', 'sqft_per_room', 'neighborhood_quality',\n",
    "                   'lat_long_interaction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine tabular and image features for TRAINING\n",
    "X_tabular = train_df_final[tabular_features].copy()\n",
    "y = train_df_final['price'].values\n",
    "property_ids = train_df_final['id'].values\n",
    "\n",
    "# Handle missing values\n",
    "print(\"\\nHandling missing values in training tabular features...\")\n",
    "X_tabular = X_tabular.fillna(X_tabular.median())\n",
    "\n",
    "# Standardize tabular features (FIT on training data)\n",
    "scaler = StandardScaler()\n",
    "X_tabular_scaled = scaler.fit_transform(X_tabular)\n",
    "X_tabular_scaled_df = pd.DataFrame(X_tabular_scaled, columns=tabular_features)\n",
    "\n",
    "# Combine all features for training\n",
    "X_combined = pd.concat([X_tabular_scaled_df, image_features_df], axis=1)\n",
    "\n",
    "print(f\"\\nTraining - Final feature set:\")\n",
    "print(f\"  - Tabular features: {len(tabular_features)}\")\n",
    "print(f\"  - Image features: {image_features_df.shape[1]}\")\n",
    "print(f\"  - Total features: {X_combined.shape[1]}\")\n",
    "print(f\"  - Samples: {X_combined.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process TEST data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMBINING FEATURES - TEST DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine tabular and image features for TEST\n",
    "X_tabular_test = test_df_final[tabular_features].copy()\n",
    "test_property_ids = test_df_final['id'].values\n",
    "\n",
    "# Handle missing values (use training median)\n",
    "print(\"\\nHandling missing values in test tabular features...\")\n",
    "for col in X_tabular_test.columns:\n",
    "    if X_tabular_test[col].isnull().any():\n",
    "        X_tabular_test[col] = X_tabular_test[col].fillna(X_tabular[col].median())\n",
    "\n",
    "# Standardize test tabular features (TRANSFORM using training scaler - DO NOT FIT)\n",
    "X_tabular_test_scaled = scaler.transform(X_tabular_test)\n",
    "X_tabular_test_scaled_df = pd.DataFrame(X_tabular_test_scaled, columns=tabular_features)\n",
    "\n",
    "# Combine all features for test\n",
    "X_combined_test = pd.concat([X_tabular_test_scaled_df, test_image_features_df], axis=1)\n",
    "\n",
    "print(f\"\\nTest - Final feature set:\")\n",
    "print(f\"  - Tabular features: {len(tabular_features)}\")\n",
    "print(f\"  - Image features: {test_image_features_df.shape[1]}\")\n",
    "print(f\"  - Total features: {X_combined_test.shape[1]}\")\n",
    "print(f\"  - Samples: {X_combined_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAIN-VALIDATION SPLIT (FROM TRAINING DATA ONLY)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split training data (80-20) for model validation during training\n",
    "X_train, X_val, y_train, y_val, ids_train, ids_val, images_train, images_val = train_test_split(\n",
    "    X_combined, y, property_ids, X_images,\n",
    "    test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training set (80% of training data):\")\n",
    "print(f\"  - Features: {X_train.shape}\")\n",
    "print(f\"  - Images: {images_train.shape}\")\n",
    "print(f\"  - Target: {y_train.shape}\")\n",
    "\n",
    "print(f\"\\nValidation set (20% of training data):\")\n",
    "print(f\"  - Features: {X_val.shape}\")\n",
    "print(f\"  - Images: {images_val.shape}\")\n",
    "print(f\"  - Target: {y_val.shape}\")\n",
    "\n",
    "print(f\"\\nTest set (separate test data - NO SPLIT):\")\n",
    "print(f\"  - Features: {X_combined_test.shape}\")\n",
    "print(f\"  - Images: {X_images_test.shape}\")\n",
    "print(f\"  - Target: NOT AVAILABLE (to be predicted)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check price distribution in splits\n",
    "print(f\"\\nPrice distribution:\")\n",
    "print(f\"Training - Mean: ${y_train.mean():,.2f}, Std: ${y_train.std():,.2f}\")\n",
    "print(f\"Validation - Mean: ${y_val.mean():,.2f}, Std: ${y_val.std():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING PREPROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create directory for processed data\n",
    "os.makedirs('preprocessed_data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TRAINING split numpy arrays\n",
    "np.save('preprocessed_data/X_train.npy', X_train.values)\n",
    "np.save('preprocessed_data/X_val.npy', X_val.values)\n",
    "np.save('preprocessed_data/y_train.npy', y_train)\n",
    "np.save('preprocessed_data/y_val.npy', y_val)\n",
    "np.save('preprocessed_data/images_train.npy', images_train)\n",
    "np.save('preprocessed_data/images_val.npy', images_val)\n",
    "np.save('preprocessed_data/ids_train.npy', ids_train)\n",
    "np.save('preprocessed_data/ids_val.npy', ids_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TEST data numpy arrays\n",
    "np.save('preprocessed_data/X_test.npy', X_combined_test.values)\n",
    "np.save('preprocessed_data/images_test.npy', X_images_test)\n",
    "np.save('preprocessed_data/ids_test.npy', test_property_ids)\n",
    "\n",
    "print(\"Training, validation, and test data saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature names and scaler\n",
    "import pickle\n",
    "\n",
    "with open('preprocessed_data/feature_names.pkl', 'wb') as f:\n",
    "    pickle.dump(X_combined.columns.tolist(), f)\n",
    "\n",
    "with open('preprocessed_data/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open('preprocessed_data/tabular_features.pkl', 'wb') as f:\n",
    "    pickle.dump(tabular_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata\n",
    "metadata = {\n",
    "    'n_train_samples': len(X_combined),\n",
    "    'n_features': X_combined.shape[1],\n",
    "    'n_tabular_features': len(tabular_features),\n",
    "    'n_image_features': image_features_df.shape[1],\n",
    "    'img_size': IMG_SIZE,\n",
    "    'train_size': len(X_train),\n",
    "    'val_size': len(X_val),\n",
    "    'test_size': len(X_combined_test),\n",
    "    'price_mean': y_train.mean(),\n",
    "    'price_std': y_train.std()\n",
    "}\n",
    "\n",
    "with open('preprocessed_data/metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All preprocessed data saved successfully!\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  TRAINING SPLIT:\")\n",
    "print(\"    - X_train.npy, X_val.npy\")\n",
    "print(\"    - y_train.npy, y_val.npy\")\n",
    "print(\"    - images_train.npy, images_val.npy\")\n",
    "print(\"    - ids_train.npy, ids_val.npy\")\n",
    "print(\"  TEST DATA:\")\n",
    "print(\"    - X_test.npy\")\n",
    "print(\"    - images_test.npy\")\n",
    "print(\"    - ids_test.npy\")\n",
    "print(\"  METADATA:\")\n",
    "print(\"    - feature_names.pkl, scaler.pkl, tabular_features.pkl, metadata.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"Ready for model training and prediction!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}